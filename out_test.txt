"**NVIDIA on GitHub ✅ success**
> Workflow [20163394335](<https://github.com/gpu-mode/discord-cluster-manager/actions/runs/20163394335>) completed
> Downloading artifacts... done
> ❌ Running benchmarks failed (internal error 1)

Running on:
* GPU: `NVIDIA B200`
* CPU: `INTEL(R) XEON(R) PLATINUM 8570`
* Runtime: `CUDA`
* Platform: `Linux-6.8.0-51-generic-x86_64-with-glibc2.35`
* Torch: `2.9.1+cu130`
# Running failed
Command ```bash
python3 eval.py benchmark /tmp/tmprp3k8_g5```
exited with error code **1** after 3.80 seconds.

## Program stderr:
```
multiprocessing.pool.RemoteTraceback: 
\"\"\"
Traceback (most recent call last):
  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker
    result = (True, func(*args, **kwds))
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py\", line 233, in _run_single_benchmark
    good, message = check_implementation(reference_output, custom_output)
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/utils.py\", line 156, in wrapped
    return match_reference(data, output, reference=reference, **kwargs)
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/utils.py\", line 142, in match_reference
    reasons = verbose_allclose(output, expected, rtol=rtol, atol=atol)
  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context
    return func(*args, **kwargs)
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/utils.py\", line 77, in verbose_allclose
    mismatched_indices = torch.nonzero(mismatched)
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

\"\"\"

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py\", line 498, in <module>
    sys.exit(main())
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py\", line 466, in main
    return run_benchmarking(logger, pool, tests)
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py\", line 325, in run_benchmarking
    result = run_single_benchmark(pool, test, False, 200, 10e9)
  File \"/home/runner/_work/discord-cluster-manager/discord-cluster-manager/eval.py\", line 304, in run_single_benchmark
    return pool.apply(_run_single_benchmark, (test, recheck, max_repeats, max_time_ns))
  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 360, in apply
    return self.apply_async(func, args, kwds).get()
  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 774, in get
    raise self._value
torch.AcceleratorError: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.```"